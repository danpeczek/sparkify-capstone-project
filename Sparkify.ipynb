{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/mini_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(dataset_path)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the churn function that will find set 1 when row means churn and 0 otherwise. To find a churn we'll use the `page` when `Downgrade` and `Cancellation Confirmation` event happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def churn(value):\n",
    "    if value == \"Downgrade\" or value == \"Cancellation Confirmation\":\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this function our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setChurn = udf(churn, IntegerType())\n",
    "df = df.withColumn(\"churn\", setChurn(df.page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's take churned users id's and let's put them to separate collection. First let's collect id of users that left our service by reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "churned_ids = df.filter(df.churn == 1).select(\"userId\").dropDuplicates().collect()\n",
    "churned_ids = [i.userId for i in churned_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define two functions that will find users that are churned and not churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "isInChurn = udf(lambda x: x in churned_ids, BooleanType())\n",
    "notInChurn = udf(lambda x: x not in churned_ids, BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many users churned from the service?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of churned users: 171\n",
      "Number of staying users: 55\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of churned users: {}\".format(df.filter(isInChurn(df.userId)).select(\"userId\").dropDuplicates().count()))\n",
    "print(\"Number of staying users: {}\".format(df.filter(notInChurn(df.userId)).select(\"userId\").dropDuplicates().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like for the sparkify mini event data more users had events `Downgraded` or `Cancellation Confirmation` rather than users which didn't take that options, however there can be users that returned to the service after leaving it (downgraded from premium and then came back for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we have null values across the dataset?\n",
    "\n",
    "Now as we have simple churn defined, let's take a look more on how the dataset looks llike. First let's see which column has a null value and how much of these values are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column artist contains 58392 null values\n",
      "Column auth contains 0 null values\n",
      "Column firstName contains 8346 null values\n",
      "Column gender contains 8346 null values\n",
      "Column itemInSession contains 0 null values\n",
      "Column lastName contains 8346 null values\n",
      "Column length contains 58392 null values\n",
      "Column level contains 0 null values\n",
      "Column location contains 8346 null values\n",
      "Column method contains 0 null values\n",
      "Column page contains 0 null values\n",
      "Column registration contains 8346 null values\n",
      "Column sessionId contains 0 null values\n",
      "Column song contains 58392 null values\n",
      "Column status contains 0 null values\n",
      "Column ts contains 0 null values\n",
      "Column userAgent contains 8346 null values\n",
      "Column userId contains 0 null values\n",
      "Column churn contains 0 null values\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(\"Column {} contains {} null values\".format(column, df.where(col(column).isNull()).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like some of the values are nulls. We can see that number of different values are the same across all columns, let's see what can be cause of this situation.\n",
    "\n",
    "Let's check if the same number null values have correclation with columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(page='Cancel'),\n",
       " Row(page='Submit Downgrade'),\n",
       " Row(page='Thumbs Down'),\n",
       " Row(page='Home'),\n",
       " Row(page='Downgrade'),\n",
       " Row(page='Roll Advert'),\n",
       " Row(page='Logout'),\n",
       " Row(page='Save Settings'),\n",
       " Row(page='Cancellation Confirmation'),\n",
       " Row(page='About'),\n",
       " Row(page='Submit Registration'),\n",
       " Row(page='Settings'),\n",
       " Row(page='Login'),\n",
       " Row(page='Register'),\n",
       " Row(page='Add to Playlist'),\n",
       " Row(page='Add Friend'),\n",
       " Row(page='Thumbs Up'),\n",
       " Row(page='Help'),\n",
       " Row(page='Upgrade'),\n",
       " Row(page='Error'),\n",
       " Row(page='Submit Upgrade')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"page\").where((col(\"artist\").isNull()) | (col(\"length\").isNull()) | (col(\"song\").isNull())).dropDuplicates().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these pages are not connected with playing a song at all, that's then understandable why the 'artist', 'length' and 'song' columns contain null values - they are not existing is such situation.\n",
    "\n",
    "Now let's see the columns for first and last name, gender and userAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(page='Home'),\n",
       " Row(page='About'),\n",
       " Row(page='Submit Registration'),\n",
       " Row(page='Login'),\n",
       " Row(page='Register'),\n",
       " Row(page='Help'),\n",
       " Row(page='Error')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"page\").where((col(\"firstName\").isNull()) | (col(\"gender\").isNull()) | (col(\"lastName\").isNull()) | (col(\"location\").isNull()) | (col(\"registration\").isNull()) | (col(\"userAgent\").isNull())).dropDuplicates().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the reason why these columns contains null values is that, the events are connected to the operations done by the users out of the logged session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's group users by their id with events sorted by timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 out of 226\n",
      "Processing 2 out of 226\n",
      "Processing 3 out of 226\n",
      "Processing 4 out of 226\n",
      "Processing 5 out of 226\n",
      "Processing 6 out of 226\n",
      "Processing 7 out of 226\n",
      "Processing 8 out of 226\n",
      "Processing 9 out of 226\n",
      "Processing 10 out of 226\n",
      "Processing 11 out of 226\n",
      "Processing 12 out of 226\n",
      "Processing 13 out of 226\n",
      "Processing 14 out of 226\n",
      "Processing 15 out of 226\n",
      "Processing 16 out of 226\n",
      "Processing 17 out of 226\n",
      "Processing 18 out of 226\n",
      "Processing 19 out of 226\n",
      "Processing 20 out of 226\n",
      "Processing 21 out of 226\n",
      "Processing 22 out of 226\n",
      "Processing 23 out of 226\n",
      "Processing 24 out of 226\n",
      "Processing 25 out of 226\n",
      "Processing 26 out of 226\n",
      "Processing 27 out of 226\n",
      "Processing 28 out of 226\n",
      "Processing 29 out of 226\n",
      "Processing 30 out of 226\n",
      "Processing 31 out of 226\n",
      "Processing 32 out of 226\n",
      "Processing 33 out of 226\n",
      "Processing 34 out of 226\n",
      "Processing 35 out of 226\n",
      "Processing 36 out of 226\n",
      "Processing 37 out of 226\n",
      "Processing 38 out of 226\n",
      "Processing 39 out of 226\n",
      "Processing 40 out of 226\n",
      "Processing 41 out of 226\n",
      "Processing 42 out of 226\n",
      "Processing 43 out of 226\n",
      "Processing 44 out of 226\n",
      "Processing 45 out of 226\n",
      "Processing 46 out of 226\n",
      "Processing 47 out of 226\n",
      "Processing 48 out of 226\n",
      "Processing 49 out of 226\n",
      "Processing 50 out of 226\n",
      "Processing 51 out of 226\n",
      "Processing 52 out of 226\n",
      "Processing 53 out of 226\n",
      "Processing 54 out of 226\n",
      "Processing 55 out of 226\n",
      "Processing 56 out of 226\n",
      "Processing 57 out of 226\n",
      "Processing 58 out of 226\n",
      "Processing 59 out of 226\n",
      "Processing 60 out of 226\n",
      "Processing 61 out of 226\n",
      "Processing 62 out of 226\n",
      "Processing 63 out of 226\n",
      "Processing 64 out of 226\n",
      "Processing 65 out of 226\n",
      "Processing 66 out of 226\n",
      "Processing 67 out of 226\n",
      "Processing 68 out of 226\n",
      "Processing 69 out of 226\n",
      "Processing 70 out of 226\n",
      "Processing 71 out of 226\n",
      "Processing 72 out of 226\n",
      "Processing 73 out of 226\n",
      "Processing 74 out of 226\n",
      "Processing 75 out of 226\n",
      "Processing 76 out of 226\n",
      "Processing 77 out of 226\n",
      "Processing 78 out of 226\n",
      "Processing 79 out of 226\n",
      "Processing 80 out of 226\n",
      "Processing 81 out of 226\n",
      "Processing 82 out of 226\n",
      "Processing 83 out of 226\n",
      "Processing 84 out of 226\n",
      "Processing 85 out of 226\n",
      "Processing 86 out of 226\n",
      "Processing 87 out of 226\n",
      "Processing 88 out of 226\n",
      "Processing 89 out of 226\n",
      "Processing 90 out of 226\n",
      "Processing 91 out of 226\n",
      "Processing 92 out of 226\n",
      "Processing 93 out of 226\n",
      "Processing 94 out of 226\n",
      "Processing 95 out of 226\n",
      "Processing 96 out of 226\n",
      "Processing 97 out of 226\n",
      "Processing 98 out of 226\n",
      "Processing 99 out of 226\n",
      "Processing 100 out of 226\n",
      "Processing 101 out of 226\n",
      "Processing 102 out of 226\n",
      "Processing 103 out of 226\n",
      "Processing 104 out of 226\n",
      "Processing 105 out of 226\n",
      "Processing 106 out of 226\n",
      "Processing 107 out of 226\n",
      "Processing 108 out of 226\n",
      "Processing 109 out of 226\n",
      "Processing 110 out of 226\n",
      "Processing 111 out of 226\n",
      "Processing 112 out of 226\n",
      "Processing 113 out of 226\n",
      "Processing 114 out of 226\n",
      "Processing 115 out of 226\n",
      "Processing 116 out of 226\n",
      "Processing 117 out of 226\n",
      "Processing 118 out of 226\n",
      "Processing 119 out of 226\n",
      "Processing 120 out of 226\n",
      "Processing 121 out of 226\n",
      "Processing 122 out of 226\n",
      "Processing 123 out of 226\n",
      "Processing 124 out of 226\n",
      "Processing 125 out of 226\n",
      "Processing 126 out of 226\n",
      "Processing 127 out of 226\n",
      "Processing 128 out of 226\n",
      "Processing 129 out of 226\n",
      "Processing 130 out of 226\n",
      "Processing 131 out of 226\n",
      "Processing 132 out of 226\n",
      "Processing 133 out of 226\n",
      "Processing 134 out of 226\n",
      "Processing 135 out of 226\n",
      "Processing 136 out of 226\n",
      "Processing 137 out of 226\n",
      "Processing 138 out of 226\n",
      "Processing 139 out of 226\n",
      "Processing 140 out of 226\n",
      "Processing 141 out of 226\n",
      "Processing 142 out of 226\n",
      "Processing 143 out of 226\n",
      "Processing 144 out of 226\n",
      "Processing 145 out of 226\n",
      "Processing 146 out of 226\n",
      "Processing 147 out of 226\n",
      "Processing 148 out of 226\n",
      "Processing 149 out of 226\n",
      "Processing 150 out of 226\n",
      "Processing 151 out of 226\n",
      "Processing 152 out of 226\n",
      "Processing 153 out of 226\n",
      "Processing 154 out of 226\n",
      "Processing 155 out of 226\n",
      "Processing 156 out of 226\n",
      "Processing 157 out of 226\n",
      "Processing 158 out of 226\n",
      "Processing 159 out of 226\n",
      "Processing 160 out of 226\n",
      "Processing 161 out of 226\n",
      "Processing 162 out of 226\n",
      "Processing 163 out of 226\n",
      "Processing 164 out of 226\n",
      "Processing 165 out of 226\n",
      "Processing 166 out of 226\n",
      "Processing 167 out of 226\n",
      "Processing 168 out of 226\n",
      "Processing 169 out of 226\n",
      "Processing 170 out of 226\n",
      "Processing 171 out of 226\n",
      "Processing 172 out of 226\n",
      "Processing 173 out of 226\n",
      "Processing 174 out of 226\n",
      "Processing 175 out of 226\n",
      "Processing 176 out of 226\n",
      "Processing 177 out of 226\n",
      "Processing 178 out of 226\n",
      "Processing 179 out of 226\n",
      "Processing 180 out of 226\n",
      "Processing 181 out of 226\n",
      "Processing 182 out of 226\n",
      "Processing 183 out of 226\n",
      "Processing 184 out of 226\n",
      "Processing 185 out of 226\n",
      "Processing 186 out of 226\n",
      "Processing 187 out of 226\n",
      "Processing 188 out of 226\n",
      "Processing 189 out of 226\n",
      "Processing 190 out of 226\n",
      "Processing 191 out of 226\n",
      "Processing 192 out of 226\n",
      "Processing 193 out of 226\n",
      "Processing 194 out of 226\n",
      "Processing 195 out of 226\n",
      "Processing 196 out of 226\n",
      "Processing 197 out of 226\n",
      "Processing 198 out of 226\n",
      "Processing 199 out of 226\n",
      "Processing 200 out of 226\n",
      "Processing 201 out of 226\n",
      "Processing 202 out of 226\n",
      "Processing 203 out of 226\n",
      "Processing 204 out of 226\n",
      "Processing 205 out of 226\n",
      "Processing 206 out of 226\n",
      "Processing 207 out of 226\n",
      "Processing 208 out of 226\n",
      "Processing 209 out of 226\n",
      "Processing 210 out of 226\n",
      "Processing 211 out of 226\n",
      "Processing 212 out of 226\n",
      "Processing 213 out of 226\n",
      "Processing 214 out of 226\n",
      "Processing 215 out of 226\n",
      "Processing 216 out of 226\n",
      "Processing 217 out of 226\n",
      "Processing 218 out of 226\n",
      "Processing 219 out of 226\n",
      "Processing 220 out of 226\n",
      "Processing 221 out of 226\n",
      "Processing 222 out of 226\n",
      "Processing 223 out of 226\n",
      "Processing 224 out of 226\n",
      "Processing 225 out of 226\n",
      "Processing 226 out of 226\n"
     ]
    }
   ],
   "source": [
    "users_by_id = []\n",
    "user_ids = df.select(\"userId\").dropDuplicates().collect()\n",
    "i = 1\n",
    "for row in user_ids:\n",
    "    print(\"Processing {} out of {}\".format(i, len(user_ids)))\n",
    "    users_by_id.append(df.select(\"*\").where(col(\"userId\") == row.userId).sort('ts').collect())\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we have these users in place let's spli them to two sub-groups:\n",
    "* Users that churned\n",
    "* Users that stayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
